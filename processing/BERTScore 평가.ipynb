{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bert-score) (2.6.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bert-score) (2.2.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bert-score) (4.50.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bert-score) (2.2.4)\n",
      "Requirement already satisfied: requests in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bert-score) (4.67.1)\n",
      "Collecting matplotlib (from bert-score)\n",
      "  Downloading matplotlib-3.10.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bert-score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.0.0->bert-score) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert-score)\n",
      "  Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert-score)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert-score)\n",
      "  Downloading fonttools-4.57.0-cp313-cp313-win_amd64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->bert-score)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib->bert-score)\n",
      "  Downloading pillow-11.1.0-cp313-cp313-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->bert-score)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->bert-score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->bert-score) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->bert-score) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kjsoo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Downloading matplotlib-3.10.1-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  7.9/8.1 MB 47.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 40.6 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp313-cp313-win_amd64.whl (220 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 23.9 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.1.0-cp313-cp313-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 12.7 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, bert-score\n",
      "Successfully installed bert-score-0.3.13 contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --user bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.0.0-cp37-abi3-win_amd64.whl.metadata (498 bytes)\n",
      "Downloading hf_xet-1.0.0-cp37-abi3-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 2.4/4.1 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.1/4.1 MB 13.6 MB/s eta 0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_A_1 = \"\"\"\n",
    "4Q24 Review: 매출액 컨센서스 부합, 영업이익은 하회 \n",
    "동사는 4Q24 연결기준 매출액 1조 636억원(YoY+178.0%, QoQ+20.6%), 영업이익 \n",
    "1,964억원(QoQ-5.4%, OPM 18.5%)을 기록, 컨센서스 매출액(1조 493억원)에 부합, \n",
    "영업이익(2,349억원)은 하회하는 실적을 거두었다. 매출원가에 반영된 일회성 요인 \n",
    "(CDMO 생산에 따른 PCO 증가, 3공장 초기 가동 제반비용 등)으로 원가율 목표치를 \n",
    "달성하지는 못했으며(48.9%, QoQ+1.4%p), 4분기에 집중적으로 반영된 프랑스 정부向 \n",
    "비용 등 판관비 단에서도 일회성 요인들이 발생하며 OPM의 하락으로 이어졌다. \n",
    "24년 review: 짐펜트라 미스를 유럽에서 만회   \n",
    "24년에는 동사의 기존 제품군 중 램시마IV(조정매출기준 YoY+28%) 및 트룩시마 \n",
    "(+67%)의 성장이 돋보였으며 신규 제품군 중에서는 베그젤마(+342%)가 큰 폭으로 \n",
    "성장했다. 상기 제품은 주로 항암 제품군이라는 점 이외에도, 24년 유럽 시장에서의 \n",
    "점유율 상승이 특히 돋보였다는 공통점을 갖는다. 동사는 항암 시장에서 동사의 공급 \n",
    "안정성이 경쟁력을 갖는 것으로 언급한 바 있는데, 이 역시 입찰 중심의 유럽 시장에 \n",
    "주로 해당되는 설명으로 판단된다.  \n",
    "한편, 짐펜트라 매출은 4분기 280억, 연간 366억을 달성하며 기대치보다 더딘 출시 \n",
    "첫 해를 기록했다. 짐펜트라의 성공을 위해서는 1)격변하는 PBM 주관 시장에 맞춘 \n",
    "빠른 대응, 2)IBD 적응증 내 글로벌 신약 대비 경쟁력 확보 및 3)마케팅 측면에서의 \n",
    "상대적인 약점 극복이 필요한 것으로 사료된다.  \n",
    "25년 Outlook: 더욱 다채로워지는 포트폴리오+ 신규품목 미국 성장 중요    \n",
    "합병 이후 동사의 투자포인트는 1)구조 효율화를 통한 매출원가율 하락과 수익성 개선, \n",
    "2)짐펜트라 등 차별화된 제품의 성공 및 3)안정적인 사업을 기반으로 신약/CDMO 등 \n",
    "신사업 진출이다. 이 중 IBD 시장의 경쟁 현황, 초기 단계의 신약 파이프라인, 관세 \n",
    "정책 불확실성 및 신규 CDMO 법인의 매출발생 시기 등을 고려했을 때 결국 \n",
    "25년도의 수익성이 단기적으로 기업가치의 driver로 작용할 전망이다. \n",
    "올해는 점유율 상승 여력이 큰 램시마SC의 유럽 매출 및 유플라이마, 베그젤마 등 \n",
    "신규 제품군의 미국 내 성장이 주목할 포인트로 판단된다. 유럽에서 신규 출시되는 \n",
    "4건 이상의 제품은 추가적인 동력을 제공할 전망이다. 이를 반영하여 신규 제품군의 \n",
    "비율이 약 49%를 이룰 것으로 추정(24년도 38.3%), 해당 제품군의 높은 마진율은 \n",
    "동사의 수익성에 기여가 가능하다. 동사의 4Q25 목표 원가율(20% 후반)을 bull case, \n",
    "30%초반을 base case, 30%중반을 bear case로 가정, base case를 적용하여 연간 \n",
    "매출액은 4조 2,631억원(YoY+19.8%), 영업이익 1조 3,515억원(OPM 31.7%)으로 \n",
    "추정한다. 이는 동사의 각 품목 연간 목표치 대비 보수적인 추정을 반영한 값이며, \n",
    "bear case를 적용해도 EPS 성장으로 보정한 동사의 벨류에이션은 섹터 내 대형주 \n",
    "중에서 가장 매력적인 수준이다.  \n",
    "투자의견 Buy 유지, 목표주가 250,000원으로 소폭 하향      \n",
    "동사에 대해 투자의견 Buy, 목표주가를 250,000원으로 소폭 하향한다. 4분기 실적을 \n",
    "반영하여 목표주가 산출의 근거가 된 ‘26년 추정 EBITDA를 2조 1,174억원으로 소폭 \n",
    "하향(기존 2조 3,613억원)하였으며 할인기간을 조정하였다. 짐펜트라에 집중된 동사 \n",
    "실적의 불확실성으로 주가는 박스권을 형성하였으나 향후 이익 성장에 대한 보수적인 \n",
    "추정을 반영해도 상승 여력은 충분한 것으로 판단된다. \n",
    "\"\"\"\n",
    "\n",
    "report_A_2 = \"\"\"\n",
    "4Q24 Review: 일회성 요인으로 영업이익 컨센 하회  \n",
    "셀트리온의 24년 4분기 매출액은 1조 636억원(+178%YoY, +20.6%QoQ), 영\n",
    "업이익은 1,964억원(+967.4%YoY, -5.4%QoQ, OPM 18.5%)으로 컨센서스 매\n",
    "출액 1조 493억원에 부합, 컨센서스 영업이익 2,349억원 대비 16.4% 하회하는 실\n",
    "적을 발표했다.  \n",
    "견조한 매출 성장에도 불구하고 1)3공장 상업 생산 개시(24년 12월)에 따른 초기 운\n",
    "영비의 증가, 2)규제기관 공장 감사에 따른 일회성 비용 발생, 3)신제품 매출 확대를 위\n",
    "한 해외 판관비 증가 및 4)3공장과 해외 인력 채용에 따른 인건비 증가 등 4분기 원\n",
    "가(5,200억원, +24.2%QoQ) 및 판관비(3,470억원, +35.7% QoQ)의 증가로 아\n",
    "쉬운 실적을 기록했다. 시장에서 주목하고 있는 짐펜트라 매출은 280억원\n",
    "(+338%QoQ)을 기록, 다소 아쉬운 상황이나 유플라이마(1,082억원, \n",
    "+157.6%YoY, +14.6%QoQ), 베그젤마(770억원, +413.3%YoY, +11.3%QoQ)\n",
    "등 신규 포트폴리오의 고성장 및 CMO 매출(1,060억원)이 매출 성장을 견인했다.  \n",
    " \n",
    "25년 수익성 개선의 원년, 영업이익 170% 성장 기대  \n",
    "셀트리온의 25년 매출액은 4조 4,192억원(+24.2%YoY), 영업이익은 1조 3,291억\n",
    "원(+170.1%YoY, OPM 30.1%)으로 전망한다.  \n",
    "1)25년에도 유플라이마(5,916억원, +69%YoY), 베그젤마(3,552억원, +61%YoY), \n",
    "스테키마(1,505억원, +653% YoY), 짐펜트라(2,541억원, +594%YoY) 등 고마진 \n",
    "신규 포트폴리오의 고성장이 기대되는 가운데 2)미국내 시밀러 시장 개화에 따른 경쟁\n",
    "심화 우려에도 불구 셀트리온의 미국 매출 비중은 지속적으로 증가하고 있으며 3)24\n",
    "년 합병에 따른 PPA 상각비의 기저효과로 원가율 개선(-12%p YoY)이 가능하다는 \n",
    "점에 긍정적인 주가 흐름을 예상, 투자의견 매수를 유지한다. 다만 25년 추정 \n",
    "EBITDA를 기존 1조 6,730억원에서 1조 6,030억원으로 하향 조정함에 따라 목표\n",
    "주가는 25만원에서 23만원으로 하향한다. \n",
    "\"\"\"\n",
    "\n",
    "report_A_3 = \"\"\"\n",
    "투자의견 매수, 목표주가 240,000원 유지 \n",
    "25년 다수의 바이오시밀러 신제품 출시가 예정되어 있으며, 원가율 하락 및 PPA \n",
    "상각비 기저효과에 따른 이익 개선 본격화 전망. 또한 합병 효과로 유럽 시장 성장\n",
    "이 견조하고, 짐펜트라를 포함한 신제품의 미국향 매출 상승이 기대되어, 동사에 \n",
    "대한 투자의견 매수, 목표주가 240,000원 유지 \n",
    "목표주가는 SOTP 방식으로 산출. 영업가치는 합병 후 원가율 정상화 및 미국 출\n",
    "시 신제품 매출 비중이 높아진 26년 EBITDA를 할인하고, 19년 램시마SC 유럽 출\n",
    "시 당시 Fwd 12m EV/EBITDA를 22% 할인한 26배 적용하여 49.3조원 추정. 순\n",
    "차입금을 반영하여 총 기업가치 49.3조원으로 산정 \n",
    " \n",
    "4Q24 Review: 3공장 초기 가동에 따른 일회성 비용으로 컨센서스 하회 \n",
    "[연결] 매출 1조 636억원(+20.6% QoQ), 영업이익 1,964억원(-5.4% QoQ), OPM \n",
    "18.5% 기록. 매출액은 후속 제품 매출 성장 및 TEVA 향 CMO 매출 약 1,064억\n",
    "원이 반영되어 컨센서스 부합. 영업이익은 3공장 가동 초기 운영에 따른 일회성 \n",
    "비용 반영으로 컨센서스 하회, 다만 25년부터는 해당 비용은 자산으로 반영 예정 \n",
    "[기존 제품] 매출 4,702억원(+0.6% QoQ)로 여전히 유럽 시장에서 안정적인 성장\n",
    "세 유지. [후속 제품] 매출 4,030억원(+17.7% QoQ)로 유플라이마 944억원\n",
    "(+18.0% QoQ), 짐펜트라 64억원(+190.9% QoQ), 스테키마 16억원(+%QoQ) 달\n",
    "성. 유플라이마는 미국에서 3분기 5-60억원대비 4분기 250억원 매출 발생. 이는 \n",
    "Low WAC 제품 출시에 따른 가격 정책 다변화 효과로 판단. 짐펜트라는 PBM 등\n",
    "재 후 도매상 계약이 지속되면서 초기 발주량 증가로 양호한 실적 기록  \n",
    " \n",
    "25년 전망: 유럽과 미국 동반 성장으로 영업이익 1.5조원 달성 기대 \n",
    "25년 매출 4조 8,829억원(+37.3% YoY), 영업이익 1조 5,117억원(+207.3% \n",
    "YoY), OPM 31.0%로 본격적인 이익 확대 구간에 진입할 것으로 전망 \n",
    "24년 짐펜트라 연간 매출 373억원으로 아쉬운 실적을 기록하였으나, 미국 내 영\n",
    "업 조직 안정화 및 PBM 연계 보험사 Formulary 등재 확대되며, 처방량 및 제품 \n",
    "출하량 증가 추세. 또한 연내 5개 바이오시밀러 신제품이 유럽 및 미국 시장에 순\n",
    "차적으로 출시되며 매출 확대에 기여할 것으로 기대\n",
    "\"\"\"\n",
    "\n",
    "report_A_4 =\"\"\"\n",
    "4Q24Re: 비용 증가하며 아쉬운 실적 \n",
    "연결 기준 매출액 1조 636억원(+178.0%YoY, 이하 YoY 생략), 영업이익 1,964억원\n",
    "(+967.4%, OPM 18.5%)로 영업 이익은 컨센서스를 하회. 3공장 상업 생산이 시작되며 관\n",
    "련 인건비, 준비 비용 등 운영 비용 증가와 일부 1회성 비용 등의 영향. 4Q24 제품 원가율\n",
    "은 49% 수준으로 3공장 가동 시작에 따른 영향을 제외할 경우 45% 수준으로 하락. 25년\n",
    "부터는 허쥬마 시밀러의 상각 비용이 반영되지 않으며, 램시마 관련 상각 비용도 25년말부\n",
    "터는 반영되지 않으면서 원가율 개선에 기여할 것으로 전망. 램시마IV, 트룩시마 등 기존 제\n",
    "품들이 견조한 점유율 증가를 이어가는 동시에 램시마SC(유럽), 베그젤마, 유플라이마 등 \n",
    "신규 제품 매출 비중도 커지고 있어 25년 이익율 개선될 것으로 판단. \n",
    "짐펜트라(미국)의 4Q24 매출은 약 280억원 수준으로 아쉬운 매출을 기록. 이는 미국 내에\n",
    "서 동사와 짐펜트라에 대한 낮은 브랜드 인지도 영향 및 미국 내 판매 채널 확대 과정에 있\n",
    "기 때문으로 판단. 24년 말부터 본격적으로 광고를 시작했으며 SC 제형의 높은 편의성 등\n",
    "을 고려할 때 25년에는 개선된 매출 기대. \n",
    "유럽과 다른 미국 시장 \n",
    "25년 최대 5개의 신규 시밀러를 순차적으로 출시할 예정. 다만, 신규 시밀러의 경우에도 유\n",
    "플라이마 시장과 비슷하게 유럽 시장에서는 빠른 시장 침투가 가능할 것으로 판단되나 미국\n",
    "의 경우 초기 시장 진입은 다소 더딜 것으로 판단. 이는 공보험 제도 및 입찰 시장 중심의 \n",
    "유럽과 달리 미국 시장에서는 사보험 중심 영향. \n",
    "투자의견 Buy, 목표주가 24만원으로 하향 \n",
    "매수 의견을 유지하며 목표 주가는 기존 25만원에서 24만원으로 소폭 하향. 목표 주가 하\n",
    "향은 25년 실적 추정치를 하향에 기인. 25년 헬스케어 합병으로 인한 원가율 상승, 상각 비\n",
    "용 반영 등이 완화되며 본격적인 이익 개선 가능할 것으로 전망. \n",
    "\"\"\"\n",
    "\n",
    "report_B = \"\"\"\n",
    "### 1. 보고서 요약 (Executive Summary)\n",
    "\n",
    "셀트리온의 2024년 4분기 실적은 다양한 요인에 의해 영향을 받을 것으로 예상되며, 매출 성장 요인으로는 기존 제품의 시장 점유율 확대와 신규 제품의 출시가 있다. 특히 후속 바이오시밀러 제품의 성장이 두드러지며, 짐펜트라의 경우 미국에서의 처방약급여관리업체(PBM) 커버리지 확보가 긍정적인 요소로 작용할 것으로 보인다. 그러나, 짐펜트라의 초기 매출 부진과 높은 연구개발(R&D) 및 생산 비용 증가가 실적에 부정적인 영향을 미칠 수 있다.\n",
    "\n",
    "셀트리온은 현재 9개의 바이오시밀러 제품 포트폴리오를 보유하고 있으며, '옴리클로', '아이덴젤트', '스테키마' 등 신규 제품의 허가를 획득하여 글로벌 시장 점유율 확대를 기대하고 있다. R&D 분야에서는 CT-P47 및 CT-P51의 임상 결과가 긍정적으로 나타나 향후 시장 진출 기대가 커지고 있다. 또한, CMO 사업을 통해 신규 수익원을 창출하고 있으며, OTC 영업 양도를 통해 사업 구조를 재편하고 있다.\n",
    "\n",
    "셀트리온은 미국 및 유럽 시장에서의 활동을 강화하고 있으며, 2030년까지 22개 제품 라인업 구축을 목표로 하고 있다. 향후 2025년에는 11개 바이오시밀러 제품의 허가를 계획하고 있으며, 자가면역질환 치료제 '짐펜트라'와 CDMO 사업의 확장을 통해 지속 가능한 성장을 추구하고 있다.\n",
    "\n",
    "향후 실적에 영향을 미칠 변수로는 신제품 성과, R&D 진행 상황, 시장 환경 변화 등이 있으며, 셀트리온은 지속적인 연구개발 투자와 글로벌 파트너십을 통해 기업 가치를 높이는 데 주력하고 있다.\n",
    "\n",
    "### 2. 2024년 4분기 실적 분석\n",
    "\n",
    "셀트리온의 2024년 4분기 실적은 여러 요인에 의해 영향을 받을 것으로 예상된다. 다음은 4분기 실적의 주요 변동 요인에 대한 분석이다.\n",
    "\n",
    "1. **매출 성장 요인**:\n",
    "    - 셀트리온은 기존 제품의 시장 점유율 확대와 신규 입찰 수주가 이어지고 있으며, 특히 후속 바이오시밀러 제품의 성장이 고무적이다. 장민환 iM증권 연구원에 따르면, 신규 출시되는 5건의 품목이 추가되면 후속 제품 매출 비중이 약 48%로 증가할 것으로 전망된다. 이는 매출 증가에 긍정적인 영향을 미칠 것으로 보인다.\n",
    "    - 짐펜트라의 경우, 미국에서의 처방약급여관리업체(PBM) 커버리지 확보가 진행되면서 매출이 본격적으로 성장할 것으로 기대된다. 현재 3대 PBM의 처방집에 모두 등재되어 있어, 향후 성과에 대한 기대가 높아지고 있다.\n",
    "2. **매출 부진 요인**:\n",
    "    - 짐펜트라의 2·3분기 매출이 100억원을 밑돌며 시장 기대에 미치지 못하고 있는 상황이다. 이는 낮은 브랜드 인지도와 주요 처방약급여관리업체(PBM) 등재의 지연이 원인으로 분석되고 있다. 이런 부정적인 요소는 4분기 실적에도 영향을 미칠 수 있다.\n",
    "3. **비용 요인**:\n",
    "    - 셀트리온은 R&D 및 생산 역량 강화를 위한 지속적인 투자 외에도, 위탁생산(CMO) 사업 진입 등을 통해 비용 구조 개선을 모색하고 있다. 그러나 이러한 비용 증가가 단기적으로는 실적에 부정적인 영향을 미칠 가능성도 존재한다.\n",
    "4. **일회성 손익**:\n",
    "    - 4분기 동안 특정 일회성 손익이 발생할 가능성이 있으며, 이는 전체 실적에 영향을 줄 수 있다. 예를 들어, CMO 사업 관련 계약 체결 및 매출 발생 등이 일회성으로 나타날 수 있다.\n",
    "\n",
    "이러한 요인들은 셀트리온의 2024년 4분기 실적에 복합적으로 작용할 것으로 예상되며, 특히 신규 제품의 판매 성과와 기존 제품의 점유율 유지가 중요한 변수가 될 것이다.\n",
    "\n",
    "### 3. 주요 사업 및 제품 동향 (Business & Product Developments)\n",
    "\n",
    "### 제품 관련 소식\n",
    "\n",
    "셀트리온은 현재 9개의 바이오시밀러 제품 포트폴리오를 보유하고 있으며, 최근 '옴리클로', '아이덴젤트', '스테키마'와 같은 신규 제품에 대한 허가를 획득하였습니다. 이 외에도 류마티스 관절염 치료제 '악템라', 골다공증 치료제 '프롤리아', 다발성경화증 치료제 '오크레부스' 등 후속 바이오시밀러 제품의 허가 절차가 진행되고 있습니다. 이러한 제품들은 셀트리온의 글로벌 시장 점유율 확대에 기여할 것으로 기대됩니다.\n",
    "\n",
    "특히, 셀트리온은 자가면역질환 치료 신약 '짐펜트라'의 미국 시장 진출 이후, 처방약급여관리업체(PBM)와의 커버리지를 확대하여 성과를 내고 있으며, 이 제품은 향후 매출 성장의 중요한 동력으로 기대되고 있습니다. 짐펜트라는 3대 PBM 모두의 처방 목록에 등재되어 있어 시장에서의 입지를 강화하고 있습니다.\n",
    "\n",
    "### R&D 및 파이프라인\n",
    "\n",
    "셀트리온은 연구개발에 연간 매출액의 약 20%를 투자하고 있으며, 바이오시밀러 파이프라인 확대와 항체의약품 신약 개발을 지속적으로 추진하고 있습니다. 최근에는 CT-P47 (악템라 바이오시밀러)의 글로벌 임상 3상 결과에서 오리지널 의약품 대비 동등성 및 유사성을 확인하였으며, 이 결과를 바탕으로 해외 주요 국가에서 허가 신청을 가속화할 계획입니다. 또한, CT-P51 (키트루다 바이오시밀러)의 유럽 임상 3상 시험계획이 동시 승인됨에 따라, 향후 시장 진출 기대가 높아지고 있습니다.\n",
    "\n",
    "### CMO 사업\n",
    "\n",
    "셀트리온은 기존의 CMO 사업을 한층 강화하기 위해 새로운 의약품 위탁개발생산(CDMO) 자회사를 설립하였습니다. 이 자회사는 신약 후보물질 선별부터 상업 생산까지 전 주기 서비스를 제공할 예정이며, 셀트리온의 항체 개발 및 생산 노하우를 활용하여 경쟁력을 높일 계획입니다. 이는 셀트리온의 CDMO 사업 진출을 본격화하는 중요한 단계로, 향후 글로벌 바이오의약품 수요 확대에 대응할 수 있는 기반을 마련할 것입니다.\n",
    "\n",
    "### 기타 사업\n",
    "\n",
    "셀트리온은 최근 3개국(한국, 홍콩, 대만)에서 OTC(일반의약품) 영업 양도 계약을 체결한 바 있습니다. 이를 통해 핵심사업에 집중하고 사업 구조를 재편하여 기업 가치를 제고하는 목표를 가지고 있습니다. 이와 같은 전략은 셀트리온의 재무구조 개선에도 긍정적인 영향을 미칠 것으로 기대됩니다.\n",
    "\n",
    "이와 같은 주요 사업 및 제품 동향은 셀트리온의 지속적인 성장과 글로벌 시장에서의 경쟁력 강화를 위한 중요한 요소로 작용하고 있습니다.\n",
    "\n",
    "### 4. 시장 환경 및 전략 방향 (Market Environment & Strategy)\n",
    "\n",
    "### 주요 시장 활동\n",
    "\n",
    "셀트리온은 2024년 4분기 동안 주요 시장인 미국과 유럽에서 활발한 활동을 전개하고 있습니다. 특히, 독일 시장에서 램시마 제품군(IV∙SC)의 점유율이 71%에 달하며, 이는 셀트리온의 제품 경쟁력을 강화하는 데 중요한 역할을 하고 있습니다. 스테키마의 출시를 통해 셀트리온은 항체 바이오 의약품 분야에서의 입지를 더욱 확고히 하고 있으며, 자가면역질환 포트폴리오를 확장하여 의료진과 환자 선택권을 높이고 있습니다.\n",
    "\n",
    "또한, 호주 시장에서도 램시마SC의 판매가 증가하고 있으며, 현지 법인이 주요 이해관계자와의 네트워크를 강화하고 임상 데이터를 알리면서 처방 선호도를 높이고 있습니다. 이러한 현지화 전략은 셀트리온의 글로벌 시장 점유율 확대에 긍정적인 영향을 미치고 있습니다.\n",
    "\n",
    "### 회사의 공식 전략\n",
    "\n",
    "셀트리온은 2024년 12월에 유럽 30개국에서의 판매 승인 권고를 획득한 바이오시밀러 제품을 통해 글로벌 시장에서의 경쟁력을 강화하고 있습니다. 회사는 2030년까지 22개 제품 라인업 구축을 목표로 하며, 바이오시밀러 시장의 높은 성장세를 활용하여 지속적으로 시장 점유율을 확대할 계획입니다.\n",
    "\n",
    "셀트리온은 CDMO(의약품 위탁개발생산) 사업 진출을 통해 신규 수익원을 창출하고, 의약품 개발 전주기 서비스를 제공하는 신규 자회사를 설립하였습니다. 이 자회사는 신약 후보물질 선별부터 상업 생산까지 모든 단계를 지원하며, 경쟁력 있는 생산성과 원가 절감에 중점을 두고 운영될 것입니다.\n",
    "\n",
    "또한, 셀트리온은 베트남 시장에 진출하여 유플라이마, 베그젤마, 옴리클로 등의 바이오시밀러 제품을 출시할 예정이며, 현지 네트워크 구축과 영업 활동을 통해 시장 점유율을 높이는 전략을 추진하고 있습니다.\n",
    "\n",
    "이와 같은 전략적 방향은 셀트리온이 글로벌 바이오의약품 시장에서 지속 가능한 성장을 이루는 데 중요한 요소로 작용할 것입니다.\n",
    "\n",
    "### 5. 향후 전망 (공식 발표 기반)\n",
    "\n",
    "셀트리온은 향후 2025년 사업 계획과 목표에 대해 구체적인 비전을 제시하고 있습니다. 회사는 2025년까지 바이오시밀러 사업 부문에서 11개 제품의 허가를 획득하고, 2030년까지 22개 제품의 포트폴리오를 확립하여 시장 지배력을 강화할 계획입니다. 이는 자가면역질환 치료제뿐만 아니라 천식, 두드러기, 안과, 대사성 골질환 등 다양한 치료 영역으로의 확장을 포함합니다. 이를 통해 다제품 전략을 통해 처방약급여관리업체(PBM)와의 협상력을 강화하고 판매 효율성을 높이겠다는 전략을 세우고 있습니다.\n",
    "\n",
    "서정진 회장은 자가면역질환 치료제 ‘짐펜트라’가 미국 시장에서 점유율을 확대하고 있으며, 내년에는 매출 5조원 달성을 목표로 하고 있다고 밝혔습니다. 올해 목표 매출인 2500억원은 충분히 달성 가능할 것으로 전망하고 있으며, 다른 바이오시밀러 제품들도 주요 시장에서 점유율을 꾸준히 확대하고 있다는 자신감을 드러냈습니다.\n",
    "\n",
    "또한, 셀트리온은 CDMO(위탁개발생산) 사업을 본격적으로 추진하고 있으며, 연내 자회사를 설립하여 CDMO 사업의 경쟁력을 높이는 데 집중할 계획입니다. 이 사업은 2028년부터 매출이 발생할 것으로 예상하고 있으며, 항체, 이중항체, 펩타이드 등 다양한 서비스를 제공하여 글로벌 시장에서의 입지를 강화하겠다는 목표를 가지고 있습니다.\n",
    "\n",
    "이와 함께 셀트리온은 신규 제조소 확보와 관련하여 결정은 연내 마무리 짓겠다고 밝혔으며, 이를 통해 글로벌 톱티어급 생산 능력을 구축하고 의약품 공급 사이클의 모든 단계에서 고객의 요구에 맞춤형 서비스를 제공할 수 있을 것으로 기대하고 있습니다.\n",
    "\n",
    "향후 실적에 영향을 미칠 수 있는 주요 요인으로는 신제품 성과, 진행 중인 R&D의 중요성, 시장 환경의 변화 등을 포함할 수 있으며, 회사의 지속적인 성장과 발전을 위한 노력은 기업의 가치를 높이는 데 기여할 것으로 전망됩니다.\n",
    "\n",
    "### 6. 기타 참고사항\n",
    "\n",
    "1. **리스크 요인**:\n",
    "    - 셀트리온의 사업 전략 및 성과는 글로벌 시장의 경쟁 환경, 규제 변화, 그리고 경제적 요인에 따라 영향을 받을 수 있습니다. 특히, 바이오시밀러 제품의 경우, 경쟁사가 출시하는 유사 제품의 개발 속도와 시장 반응이 중요한 변수로 작용할 수 있습니다.\n",
    "    - 또한, 임상시험 및 제품 허가 과정에서 발생할 수 있는 지연이나 실패는 재무적 측면에서 부정적인 영향을 미칠 수 있습니다.\n",
    "2. **기회 요인**:\n",
    "    - 셀트리온은 바이오시밀러와 신약 개발 부문에서 지속적인 연구개발 투자로 새로운 치료 옵션을 제공할 수 있는 기회를 갖고 있습니다. 특히, 다중항체 플랫폼 기술 개발과 같은 혁신적인 접근 방식은 향후 시장에서의 경쟁력을 강화할 가능성이 있습니다.\n",
    "    - 글로벌 시장에서의 판매 확대 및 파트너십 체결을 통해 매출 성장을 도모할 수 있는 잠재적인 기회가 존재합니다.\n",
    "3. **전략적 제휴 및 파트너십**:\n",
    "    - 셀트리온은 여러 글로벌 제약사와의 협력 및 라이선스 계약을 통해 개발 비용을 분담하고 새로운 시장에 진입할 수 있는 기회를 모색하고 있습니다.\n",
    "    - 이러한 전략적 제휴는 기술력 향상과 시장 점유율 확대에 기여할 수 있습니다.\n",
    "4. **주주 가치 제고**:\n",
    "    - 셀트리온은 2024년 내 자기주식 매입을 검토 중이며, 이를 통해 주주 가치를 증대시킬 계획을 세우고 있습니다.\n",
    "5. **기타 중요 사항**:\n",
    "    - 셀트리온의 연구개발 투자 비율은 매출액의 약 20%에 달하며, 이는 바이오시밀러 및 신약 개발에 대한 의지를 반영합니다.\n",
    "    - 회사는 향후 중장기적인 사업 계획 및 경영 전략을 지속적으로 업데이트할 예정이며, 이는 투자자들에게 중요한 정보로 작용할 수 있습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore 계산 시작 (모델: klue/bert-base, 레이어 수: 12)...\n",
      "\n",
      "--- BERTScore 평가 결과 ---\n",
      "사용된 모델: klue/bert-base\n",
      "평가 대상 Candidate: 1개\n",
      "비교 기준 References: 4개\n",
      "\n",
      "[Reference 1 과의 비교]\n",
      "BERT Precision: 0.6271\n",
      "BERT Recall:    0.5835\n",
      "BERT F1 Score:  0.6045\n",
      "\n",
      "[Reference 2 과의 비교]\n",
      "BERT Precision: 0.5875\n",
      "BERT Recall:    0.5202\n",
      "BERT F1 Score:  0.5518\n",
      "\n",
      "[Reference 3 과의 비교]\n",
      "BERT Precision: 0.6238\n",
      "BERT Recall:    0.5541\n",
      "BERT F1 Score:  0.5869\n",
      "\n",
      "[Reference 4 과의 비교]\n",
      "BERT Precision: 0.6214\n",
      "BERT Recall:    0.5814\n",
      "BERT F1 Score:  0.6007\n",
      "\n",
      "--- 평균 점수 ---\n",
      "Average Precision: 0.6149\n",
      "Average Recall:    0.5598\n",
      "Average F1 Score:  0.5860\n",
      "--- 결과 해석 ---\n",
      "Precision (62.14%): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\n",
      "Recall (58.14%): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\n",
      "F1 Score (0.60): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# BERTScore 계산을 위해 리스트 형태로 준비합니다.\n",
    "candidate_report = [report_B] # 평가 대상은 하나이므로 길이가 1인 리스트\n",
    "reference_reports = [report_A_1, report_A_2, report_A_3, report_A_4] # 참조 기준 리스트\n",
    "\n",
    "# --- 2. BERTScore 계산 ---\n",
    "model_name = \"klue/bert-base\"\n",
    "num_layers_to_use = 12\n",
    "\n",
    "print(f\"BERTScore 계산 시작 (모델: {model_name}, 레이어 수: {num_layers_to_use})...\")\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_f1_scores = []\n",
    "\n",
    "# 각 reference 보고서에 대해 개별적으로 BERTScore 계산\n",
    "for ref in reference_reports:\n",
    "    # score 함수는 입력으로 리스트를 받으므로, ref도 리스트로 감싸줍니다.\n",
    "    P, R, F1 = score(candidate_report, [ref], model_type=model_name, lang=\"ko\", num_layers=num_layers_to_use, verbose=False) # 반복문 안에서는 verbose=False가 깔끔할 수 있습니다.\n",
    "\n",
    "    # 결과 텐서에서 스칼라 값 추출 (이제 P, R, F1은 shape (1, 1) 이므로 .item() 사용 가능)\n",
    "    precision_score = P[0].item()\n",
    "    recall_score = R[0].item()\n",
    "    f1_score_value = F1[0].item()\n",
    "\n",
    "    # 계산된 점수를 각 리스트에 추가\n",
    "    all_precisions.append(precision_score)\n",
    "    all_recalls.append(recall_score)\n",
    "    all_f1_scores.append(f1_score_value)\n",
    "\n",
    "# --- 3. 결과 출력 ---\n",
    "print(\"\\n--- BERTScore 평가 결과 ---\")\n",
    "print(f\"사용된 모델: {model_name}\")\n",
    "print(f\"평가 대상 Candidate: 1개\")\n",
    "print(f\"비교 기준 References: {len(reference_reports)}개\")\n",
    "\n",
    "# 각 reference와의 비교 결과 출력\n",
    "for i in range(len(reference_reports)):\n",
    "    print(f\"\\n[Reference {i+1} 과의 비교]\")\n",
    "    print(f\"BERT Precision: {all_precisions[i]:.4f}\")\n",
    "    print(f\"BERT Recall:    {all_recalls[i]:.4f}\")\n",
    "    print(f\"BERT F1 Score:  {all_f1_scores[i]:.4f}\")\n",
    "\n",
    "# (선택) 평균 점수 계산 및 출력\n",
    "if len(reference_reports) > 0:\n",
    "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "    avg_f1 = sum(all_f1_scores) / len(all_f1_scores)\n",
    "    print(\"\\n--- 평균 점수 ---\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall:    {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score:  {avg_f1:.4f}\")\n",
    "\n",
    "print(\"--- 결과 해석 ---\")\n",
    "print(f\"Precision ({precision_score:.2%}): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\")\n",
    "print(f\"Recall ({recall_score:.2%}): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\")\n",
    "print(f\"F1 Score ({f1_score_value:.2f}): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore 계산 시작 (모델: klue/bert-base, 레이어 수: 12)...\n",
      "\n",
      "--- BERTScore 평가 결과 ---\n",
      "사용된 모델: klue/bert-base\n",
      "평가 대상 Candidate: 1개\n",
      "비교 기준 References: 3개\n",
      "\n",
      "[Reference 1 과의 비교]\n",
      "BERT Precision: 0.6610\n",
      "BERT Recall:    0.7550\n",
      "BERT F1 Score:  0.7049\n",
      "\n",
      "[Reference 2 과의 비교]\n",
      "BERT Precision: 0.6649\n",
      "BERT Recall:    0.6828\n",
      "BERT F1 Score:  0.6737\n",
      "\n",
      "[Reference 3 과의 비교]\n",
      "BERT Precision: 0.6751\n",
      "BERT Recall:    0.6619\n",
      "BERT F1 Score:  0.6684\n",
      "\n",
      "--- 평균 점수 ---\n",
      "Average Precision: 0.6670\n",
      "Average Recall:    0.6999\n",
      "Average F1 Score:  0.6823\n",
      "--- 결과 해석 ---\n",
      "Precision (67.51%): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\n",
      "Recall (66.19%): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\n",
      "F1 Score (0.67): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# BERTScore 계산을 위해 리스트 형태로 준비합니다.\n",
    "candidate_report = [report_A_1] # 평가 대상은 하나이므로 길이가 1인 리스트\n",
    "reference_reports = [report_A_2, report_A_3, report_A_4] # 참조 기준 리스트\n",
    "\n",
    "# --- 2. BERTScore 계산 ---\n",
    "model_name = \"klue/bert-base\"\n",
    "num_layers_to_use = 12\n",
    "\n",
    "print(f\"BERTScore 계산 시작 (모델: {model_name}, 레이어 수: {num_layers_to_use})...\")\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_f1_scores = []\n",
    "\n",
    "# 각 reference 보고서에 대해 개별적으로 BERTScore 계산\n",
    "for ref in reference_reports:\n",
    "    # score 함수는 입력으로 리스트를 받으므로, ref도 리스트로 감싸줍니다.\n",
    "    P, R, F1 = score(candidate_report, [ref], model_type=model_name, lang=\"ko\", num_layers=num_layers_to_use, verbose=False) # 반복문 안에서는 verbose=False가 깔끔할 수 있습니다.\n",
    "\n",
    "    # 결과 텐서에서 스칼라 값 추출 (이제 P, R, F1은 shape (1, 1) 이므로 .item() 사용 가능)\n",
    "    precision_score = P[0].item()\n",
    "    recall_score = R[0].item()\n",
    "    f1_score_value = F1[0].item()\n",
    "\n",
    "    # 계산된 점수를 각 리스트에 추가\n",
    "    all_precisions.append(precision_score)\n",
    "    all_recalls.append(recall_score)\n",
    "    all_f1_scores.append(f1_score_value)\n",
    "\n",
    "# --- 3. 결과 출력 ---\n",
    "print(\"\\n--- BERTScore 평가 결과 ---\")\n",
    "print(f\"사용된 모델: {model_name}\")\n",
    "print(f\"평가 대상 Candidate: 1개\")\n",
    "print(f\"비교 기준 References: {len(reference_reports)}개\")\n",
    "\n",
    "# 각 reference와의 비교 결과 출력\n",
    "for i in range(len(reference_reports)):\n",
    "    print(f\"\\n[Reference {i+1} 과의 비교]\")\n",
    "    print(f\"BERT Precision: {all_precisions[i]:.4f}\")\n",
    "    print(f\"BERT Recall:    {all_recalls[i]:.4f}\")\n",
    "    print(f\"BERT F1 Score:  {all_f1_scores[i]:.4f}\")\n",
    "\n",
    "# (선택) 평균 점수 계산 및 출력\n",
    "if len(reference_reports) > 0:\n",
    "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "    avg_f1 = sum(all_f1_scores) / len(all_f1_scores)\n",
    "    print(\"\\n--- 평균 점수 ---\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall:    {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score:  {avg_f1:.4f}\")\n",
    "\n",
    "print(\"--- 결과 해석 ---\")\n",
    "print(f\"Precision ({precision_score:.2%}): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\")\n",
    "print(f\"Recall ({recall_score:.2%}): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\")\n",
    "print(f\"F1 Score ({f1_score_value:.2f}): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore 계산 시작 (모델: klue/bert-base, 레이어 수: 12)...\n",
      "\n",
      "--- BERTScore 평가 결과 ---\n",
      "사용된 모델: klue/bert-base\n",
      "평가 대상 Candidate: 1개\n",
      "비교 기준 References: 3개\n",
      "\n",
      "[Reference 1 과의 비교]\n",
      "BERT Precision: 0.7550\n",
      "BERT Recall:    0.6610\n",
      "BERT F1 Score:  0.7049\n",
      "\n",
      "[Reference 2 과의 비교]\n",
      "BERT Precision: 0.7510\n",
      "BERT Recall:    0.6659\n",
      "BERT F1 Score:  0.7059\n",
      "\n",
      "[Reference 3 과의 비교]\n",
      "BERT Precision: 0.7185\n",
      "BERT Recall:    0.6062\n",
      "BERT F1 Score:  0.6576\n",
      "\n",
      "--- 평균 점수 ---\n",
      "Average Precision: 0.7415\n",
      "Average Recall:    0.6444\n",
      "Average F1 Score:  0.6894\n",
      "--- 결과 해석 ---\n",
      "Precision (71.85%): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\n",
      "Recall (60.62%): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\n",
      "F1 Score (0.66): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# BERTScore 계산을 위해 리스트 형태로 준비합니다.\n",
    "candidate_report = [report_A_2] # 평가 대상은 하나이므로 길이가 1인 리스트\n",
    "reference_reports = [report_A_1, report_A_3, report_A_4] # 참조 기준 리스트\n",
    "\n",
    "# --- 2. BERTScore 계산 ---\n",
    "model_name = \"klue/bert-base\"\n",
    "num_layers_to_use = 12\n",
    "\n",
    "print(f\"BERTScore 계산 시작 (모델: {model_name}, 레이어 수: {num_layers_to_use})...\")\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_f1_scores = []\n",
    "\n",
    "# 각 reference 보고서에 대해 개별적으로 BERTScore 계산\n",
    "for ref in reference_reports:\n",
    "    # score 함수는 입력으로 리스트를 받으므로, ref도 리스트로 감싸줍니다.\n",
    "    P, R, F1 = score(candidate_report, [ref], model_type=model_name, lang=\"ko\", num_layers=num_layers_to_use, verbose=False) # 반복문 안에서는 verbose=False가 깔끔할 수 있습니다.\n",
    "\n",
    "    # 결과 텐서에서 스칼라 값 추출 (이제 P, R, F1은 shape (1, 1) 이므로 .item() 사용 가능)\n",
    "    precision_score = P[0].item()\n",
    "    recall_score = R[0].item()\n",
    "    f1_score_value = F1[0].item()\n",
    "\n",
    "    # 계산된 점수를 각 리스트에 추가\n",
    "    all_precisions.append(precision_score)\n",
    "    all_recalls.append(recall_score)\n",
    "    all_f1_scores.append(f1_score_value)\n",
    "\n",
    "# --- 3. 결과 출력 ---\n",
    "print(\"\\n--- BERTScore 평가 결과 ---\")\n",
    "print(f\"사용된 모델: {model_name}\")\n",
    "print(f\"평가 대상 Candidate: 1개\")\n",
    "print(f\"비교 기준 References: {len(reference_reports)}개\")\n",
    "\n",
    "# 각 reference와의 비교 결과 출력\n",
    "for i in range(len(reference_reports)):\n",
    "    print(f\"\\n[Reference {i+1} 과의 비교]\")\n",
    "    print(f\"BERT Precision: {all_precisions[i]:.4f}\")\n",
    "    print(f\"BERT Recall:    {all_recalls[i]:.4f}\")\n",
    "    print(f\"BERT F1 Score:  {all_f1_scores[i]:.4f}\")\n",
    "\n",
    "# (선택) 평균 점수 계산 및 출력\n",
    "if len(reference_reports) > 0:\n",
    "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "    avg_f1 = sum(all_f1_scores) / len(all_f1_scores)\n",
    "    print(\"\\n--- 평균 점수 ---\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall:    {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score:  {avg_f1:.4f}\")\n",
    "\n",
    "print(\"--- 결과 해석 ---\")\n",
    "print(f\"Precision ({precision_score:.2%}): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\")\n",
    "print(f\"Recall ({recall_score:.2%}): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\")\n",
    "print(f\"F1 Score ({f1_score_value:.2f}): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore 계산 시작 (모델: klue/bert-base, 레이어 수: 12)...\n",
      "\n",
      "--- BERTScore 평가 결과 ---\n",
      "사용된 모델: klue/bert-base\n",
      "평가 대상 Candidate: 1개\n",
      "비교 기준 References: 3개\n",
      "\n",
      "[Reference 1 과의 비교]\n",
      "BERT Precision: 0.6828\n",
      "BERT Recall:    0.6649\n",
      "BERT F1 Score:  0.6737\n",
      "\n",
      "[Reference 2 과의 비교]\n",
      "BERT Precision: 0.6659\n",
      "BERT Recall:    0.7510\n",
      "BERT F1 Score:  0.7059\n",
      "\n",
      "[Reference 3 과의 비교]\n",
      "BERT Precision: 0.7027\n",
      "BERT Recall:    0.6807\n",
      "BERT F1 Score:  0.6916\n",
      "\n",
      "--- 평균 점수 ---\n",
      "Average Precision: 0.6838\n",
      "Average Recall:    0.6989\n",
      "Average F1 Score:  0.6904\n",
      "--- 결과 해석 ---\n",
      "Precision (70.27%): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\n",
      "Recall (68.07%): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\n",
      "F1 Score (0.69): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# BERTScore 계산을 위해 리스트 형태로 준비합니다.\n",
    "candidate_report = [report_A_3] # 평가 대상은 하나이므로 길이가 1인 리스트\n",
    "reference_reports = [report_A_1, report_A_2, report_A_4] # 참조 기준 리스트\n",
    "\n",
    "# --- 2. BERTScore 계산 ---\n",
    "model_name = \"klue/bert-base\"\n",
    "num_layers_to_use = 12\n",
    "\n",
    "print(f\"BERTScore 계산 시작 (모델: {model_name}, 레이어 수: {num_layers_to_use})...\")\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_f1_scores = []\n",
    "\n",
    "# 각 reference 보고서에 대해 개별적으로 BERTScore 계산\n",
    "for ref in reference_reports:\n",
    "    # score 함수는 입력으로 리스트를 받으므로, ref도 리스트로 감싸줍니다.\n",
    "    P, R, F1 = score(candidate_report, [ref], model_type=model_name, lang=\"ko\", num_layers=num_layers_to_use, verbose=False) # 반복문 안에서는 verbose=False가 깔끔할 수 있습니다.\n",
    "\n",
    "    # 결과 텐서에서 스칼라 값 추출 (이제 P, R, F1은 shape (1, 1) 이므로 .item() 사용 가능)\n",
    "    precision_score = P[0].item()\n",
    "    recall_score = R[0].item()\n",
    "    f1_score_value = F1[0].item()\n",
    "\n",
    "    # 계산된 점수를 각 리스트에 추가\n",
    "    all_precisions.append(precision_score)\n",
    "    all_recalls.append(recall_score)\n",
    "    all_f1_scores.append(f1_score_value)\n",
    "\n",
    "# --- 3. 결과 출력 ---\n",
    "print(\"\\n--- BERTScore 평가 결과 ---\")\n",
    "print(f\"사용된 모델: {model_name}\")\n",
    "print(f\"평가 대상 Candidate: 1개\")\n",
    "print(f\"비교 기준 References: {len(reference_reports)}개\")\n",
    "\n",
    "# 각 reference와의 비교 결과 출력\n",
    "for i in range(len(reference_reports)):\n",
    "    print(f\"\\n[Reference {i+1} 과의 비교]\")\n",
    "    print(f\"BERT Precision: {all_precisions[i]:.4f}\")\n",
    "    print(f\"BERT Recall:    {all_recalls[i]:.4f}\")\n",
    "    print(f\"BERT F1 Score:  {all_f1_scores[i]:.4f}\")\n",
    "\n",
    "# (선택) 평균 점수 계산 및 출력\n",
    "if len(reference_reports) > 0:\n",
    "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "    avg_f1 = sum(all_f1_scores) / len(all_f1_scores)\n",
    "    print(\"\\n--- 평균 점수 ---\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall:    {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score:  {avg_f1:.4f}\")\n",
    "\n",
    "print(\"--- 결과 해석 ---\")\n",
    "print(f\"Precision ({precision_score:.2%}): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\")\n",
    "print(f\"Recall ({recall_score:.2%}): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\")\n",
    "print(f\"F1 Score ({f1_score_value:.2f}): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore 계산 시작 (모델: klue/bert-base, 레이어 수: 12)...\n",
      "\n",
      "--- BERTScore 평가 결과 ---\n",
      "사용된 모델: klue/bert-base\n",
      "평가 대상 Candidate: 1개\n",
      "비교 기준 References: 3개\n",
      "\n",
      "[Reference 1 과의 비교]\n",
      "BERT Precision: 0.6619\n",
      "BERT Recall:    0.6751\n",
      "BERT F1 Score:  0.6684\n",
      "\n",
      "[Reference 2 과의 비교]\n",
      "BERT Precision: 0.6062\n",
      "BERT Recall:    0.7185\n",
      "BERT F1 Score:  0.6576\n",
      "\n",
      "[Reference 3 과의 비교]\n",
      "BERT Precision: 0.6807\n",
      "BERT Recall:    0.7027\n",
      "BERT F1 Score:  0.6916\n",
      "\n",
      "--- 평균 점수 ---\n",
      "Average Precision: 0.6496\n",
      "Average Recall:    0.6987\n",
      "Average F1 Score:  0.6725\n",
      "--- 결과 해석 ---\n",
      "Precision (68.07%): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\n",
      "Recall (70.27%): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\n",
      "F1 Score (0.69): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# BERTScore 계산을 위해 리스트 형태로 준비합니다.\n",
    "candidate_report = [report_A_4] # 평가 대상은 하나이므로 길이가 1인 리스트\n",
    "reference_reports = [report_A_1, report_A_2, report_A_3] # 참조 기준 리스트\n",
    "\n",
    "# --- 2. BERTScore 계산 ---\n",
    "model_name = \"klue/bert-base\"\n",
    "num_layers_to_use = 12\n",
    "\n",
    "print(f\"BERTScore 계산 시작 (모델: {model_name}, 레이어 수: {num_layers_to_use})...\")\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_f1_scores = []\n",
    "\n",
    "# 각 reference 보고서에 대해 개별적으로 BERTScore 계산\n",
    "for ref in reference_reports:\n",
    "    # score 함수는 입력으로 리스트를 받으므로, ref도 리스트로 감싸줍니다.\n",
    "    P, R, F1 = score(candidate_report, [ref], model_type=model_name, lang=\"ko\", num_layers=num_layers_to_use, verbose=False) # 반복문 안에서는 verbose=False가 깔끔할 수 있습니다.\n",
    "\n",
    "    # 결과 텐서에서 스칼라 값 추출 (이제 P, R, F1은 shape (1, 1) 이므로 .item() 사용 가능)\n",
    "    precision_score = P[0].item()\n",
    "    recall_score = R[0].item()\n",
    "    f1_score_value = F1[0].item()\n",
    "\n",
    "    # 계산된 점수를 각 리스트에 추가\n",
    "    all_precisions.append(precision_score)\n",
    "    all_recalls.append(recall_score)\n",
    "    all_f1_scores.append(f1_score_value)\n",
    "\n",
    "# --- 3. 결과 출력 ---\n",
    "print(\"\\n--- BERTScore 평가 결과 ---\")\n",
    "print(f\"사용된 모델: {model_name}\")\n",
    "print(f\"평가 대상 Candidate: 1개\")\n",
    "print(f\"비교 기준 References: {len(reference_reports)}개\")\n",
    "\n",
    "# 각 reference와의 비교 결과 출력\n",
    "for i in range(len(reference_reports)):\n",
    "    print(f\"\\n[Reference {i+1} 과의 비교]\")\n",
    "    print(f\"BERT Precision: {all_precisions[i]:.4f}\")\n",
    "    print(f\"BERT Recall:    {all_recalls[i]:.4f}\")\n",
    "    print(f\"BERT F1 Score:  {all_f1_scores[i]:.4f}\")\n",
    "\n",
    "# (선택) 평균 점수 계산 및 출력\n",
    "if len(reference_reports) > 0:\n",
    "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "    avg_f1 = sum(all_f1_scores) / len(all_f1_scores)\n",
    "    print(\"\\n--- 평균 점수 ---\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall:    {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score:  {avg_f1:.4f}\")\n",
    "\n",
    "print(\"--- 결과 해석 ---\")\n",
    "print(f\"Precision ({precision_score:.2%}): LLM 생성 보고서(B)의 각 단어(토큰)가 실제 보고서(A)의 단어들과 **의미적으로 얼마나 유사하고 관련성이 높은지**를 나타냅니다.\")\n",
    "print(f\"Recall ({recall_score:.2%}): 실제 보고서(A)의 각 단어(토큰)들이 LLM 생성 보고서(B)에서 **의미적으로 얼마나 잘 표현되었는지(포함되었는지)**를 나타냅니다.\")\n",
    "print(f\"F1 Score ({f1_score_value:.2f}): Precision과 Recall의 조화 평균으로, 두 보고서 간의 전반적인 **의미론적 유사도**를 종합적으로 보여주는 지표입니다. (1에 가까울수록 유사도가 높음)\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
